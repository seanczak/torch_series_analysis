{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Convolutional Arthematic for Multivariate, Multi-Output Time Series in PyTorch</h1>\n",
    "<h2> Parameterizing the Causal Autoencoder Problem Statement</h2>\n",
    "\n",
    "Explain autoencoder use\n",
    "\n",
    "figure 1: consider 1d series...show that first of target isn't in future (needs to be shifted back to be causal) and because of that last of input doesn't have a future prediction (draw line to show where to lop off both series and note that it's \"1\" value at the moment)\n",
    "\n",
    "in convolution, you actually need to provide some padding to only the front (which we'll show you how to do) to make sure you are causal, it's dependent on the receptive field of the model which is dependent on the dilation and kernel size (equation)\n",
    "\n",
    "figure 2: show some greyed out zeros on the image above and preds are now added (maybe a single kernel grabbing inputs and pointing to pred it creates)\n",
    "\n",
    "NOte that because of this, you might want to add a little priming during training before you start scoring the results (cut off the ones that were influenced by padding if you so choose, this will be easy to do though because both the targets and the preds would be the same amount to cut off - \"padding\" below)\n",
    "\n",
    "Now remember as we do this that this can easily be extended to multivariate (multi-channel) data and of course pytorch can handle batches as well. so as we do this with one example, just realize the the blocks could extend into the page like so... (just don't feel like drawing it)...theoretically the channels in are the same amount as the channels out but technically you could only want say 1 out channel for 5 in channels but of course for an autoencoder that doesn't work the other way around (although if you did have a target that was different than the input the logic in this article should help you think through the indexing of that as well)\n",
    "\n",
    "figure 3: show that the channels extend into board (maybe add another one)\n",
    "\n",
    "we may want to guess farther into future (say next month if these are values taken weekly), so let's see what that does to the image below\n",
    "\n",
    "figure 4: modify fig 2 show guessing 5? into the future (again with the lines and showing how much to lop off begin/end of respective series), greyed out bits that aren't used, note that inputs no longer have targets at the end because they got shifted, note that predictions don't need to be modified (as you cut the input len, predictions follow)\n",
    "\n",
    "or you may want to predict each week for the next 4 weeks\n",
    "\n",
    "figure 5: same as fig 2 but now outputs have been stacked\n",
    "\n",
    "or you may want to combine these (either to make it harder or because you have an actual use age for the information)\n",
    "\n",
    "figure 6: both fig 4 and 5\n",
    "\n",
    "finally a limited use case I imagine but just in case it comes up, covering bases, what if you strided your kernel. well this is basically just subsampling the outputs which means that with no modification of your input series, your preds will have fewer values so you need to also remove the corresponding values from the targets array\n",
    "\n",
    "figure 7: show stride 2 and 3 on both of the above (or just one example and say trust me it works the same for figures 2,4,5,6 equally)\n",
    "\n",
    "ok so using this reshaping logic, you should have the necessary framework to start reshaping/indexing properly to create a causal autoencoder... now I'm going to show you how this works in pytorch. Going to be laborious about the indexing because it's really important to know how to access things when you're training/rebuilding/adding-to/evaluating a model\n",
    "\n",
    "but first here are the answers (note that I want to leave padding for the individual layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make autoencoder class out of conv1d, typically means you use past to predict next or several next in a time series (or several if you have multivariate).  TYpically autoencoders are made to be used to pretrain several layers of another project but sometimes they are useful just as is (think stock market forecasting, supply/demand predictions, etc).  Regardless of your application, you may not want to stop \n",
    "\n",
    "However, deep learning aside, wrapping your mind around/keeping straight the indexing logic is enough for me to never want to do again (after doing once) so I'm writing this to keep track of it and develop the logic piecemeal. In the end we'll end up with something that was built from scratch so we know what each piece does so when you (or I) go back and want to change bits of it (say add layers), no one ever has to think through this logic again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we know that most of the deep learning field has been driven by computer vision related work so we have to modify a bit but most of the tools in pytorch are satisfactory, we just need to wrap them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that to make it causal, we pad the front bit of the input before sending into the forward pass (see wavenet implementation https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py#L23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_1dConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolution with the option to be causal and use xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 dilation=1, bias=True, w_init_gain='linear', is_causal=True):\n",
    "        super(Causal_1dConv, self).__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    dilation=dilation, bias=bias)\n",
    "\n",
    "#         torch.nn.init.xavier_uniform(\n",
    "#             self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        if self.is_causal:\n",
    "                padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
    "                signal = torch.nn.functional.pad(signal, padding) \n",
    "        return self.conv(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the `torch.nn.Conv1d` class pads both sides which doesn't make sense for us, so we still us the class but add the padding onto the front of the input ourselves (turns out the function `torch.nn.functional.pad` allows you to specify how much to put on each end with a `tuple` argument instead of an `int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n",
      "Padded:   tensor([ 0,  0,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "signal = torch.arange(10)+5\n",
    "print('Original:',signal)\n",
    "padding = (2,0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "print('Padded:  ',signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that the kernel will be of the shape `out_channels, in_channels, kernel_size` because what it really does is create multiple (`out_channels`) kernels of size `in_channels, kernel_size` and so your output is of the shape `batch_size, out_channels, seq_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "dilation = 1\n",
    "stride = 1\n",
    "out_channels = 4\n",
    "in_channels = 2\n",
    "\n",
    "conv1d_layer = Causal_1dConv(in_channels = in_channels, \n",
    "                             out_channels = out_channels,\n",
    "                             kernel_size = kernel_size, \n",
    "                             bias = False,\n",
    "                             dilation = dilation,\n",
    "                            stride = stride)\n",
    "\n",
    "# make weights be an averaging function or simple sum\n",
    "conv1d_layer.conv.weight.data = torch.ones(out_channels,in_channels,kernel_size)\n",
    "conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to change the output of the second channel to be twice as great as the others (remember our kernel is all 1's right now so it basically just sums up everything within it). Note that these are definitely not what the kernels of any model would look like but their simplicity allow for our human brains to quickly figure out what the output should be and thus give a sense for what the model is doing under the hood. For example, the kernels of 1's simply mean that the 6 numbers it touches will be added together. This way we can make sure that the results are, say, causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer.conv.weight.data[1] = 2\n",
    "conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect on that, we have to create some fake data. Let's create two channels that basically just count up from 0 by 10s. The first channel will do that but the second will just be offset by 5.  Finally, to show that pytorch can handle (and in fact, expects) batches, we can create another example that is simply shifted up by 100 and when we combine the two we have a \"mini batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Example:\n",
      "tensor([[[ 0., 10., 20., 30., 40., 50.],\n",
      "         [ 5., 15., 25., 35., 45., 55.]]])\n",
      "torch.Size([1, 2, 6])\n",
      "\n",
      "2nd Example:\n",
      "tensor([[[100., 110., 120., 130., 140., 150.],\n",
      "         [105., 115., 125., 135., 145., 155.]]])\n",
      "torch.Size([1, 2, 6])\n",
      "\n",
      "Batched Examples:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150.],\n",
      "         [105., 115., 125., 135., 145., 155.]]])\n",
      "torch.Size([2, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 6\n",
    "channels = 2\n",
    "\n",
    "example1 = torch.zeros((1,channels,seq_len)) + torch.arange(seq_len).float() * 10\n",
    "example1[:,1] = example1[:,1] + 5\n",
    "print(f'1st Example:\\n{example1}\\n{example1.shape}\\n')\n",
    "\n",
    "example2 = example1 + 100\n",
    "print(f'2nd Example:\\n{example2}\\n{example2.shape}\\n')\n",
    "\n",
    "examples = torch.cat((example1, example2), 0)\n",
    "print(f'Batched Examples:\\n{examples}\\n{examples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing it through, let's remind ourselves of what the model is going to see after padding (notice pytorch has no problem with padding multiple channels and multiple examples, if you only pass a tuple of shape (2,) it will only pad the final dimension (which for us is time). The 0 as the second element of the padding input tells it we don't need to pad the end of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150.],\n",
      "         [105., 115., 125., 135., 145., 155.]]])\n",
      "\n",
      "Padded:\n",
      "tensor([[[  0.,   0.,   0.,  10.,  20.,  30.,  40.,  50.],\n",
      "         [  0.,   0.,   5.,  15.,  25.,  35.,  45.,  55.]],\n",
      "\n",
      "        [[  0.,   0., 100., 110., 120., 130., 140., 150.],\n",
      "         [  0.,   0., 105., 115., 125., 135., 145., 155.]]])\n"
     ]
    }
   ],
   "source": [
    "signal = examples\n",
    "print(f'Original:\\n{signal}\\n')\n",
    "padding = (int((kernel_size - 1) * (dilation)), 0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "print(f'Padded Input:\\n{signal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can see that we just add the numbers up as the kernel observes them and because of the padding, our output `seq_len` matches our input `seq_len`. In addition, we can see that since we modified the second channel's kernel to be 2's instead of 1's, it is like adding the numbers and then multiplying by 2 for the second channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   5.,   30.,   75.,  135.,  195.,  255.],\n",
       "         [  10.,   60.,  150.,  270.,  390.,  510.],\n",
       "         [   5.,   30.,   75.,  135.,  195.,  255.],\n",
       "         [   5.,   30.,   75.,  135.,  195.,  255.]],\n",
       "\n",
       "        [[ 205.,  430.,  675.,  735.,  795.,  855.],\n",
       "         [ 410.,  860., 1350., 1470., 1590., 1710.],\n",
       "         [ 205.,  430.,  675.,  735.,  795.,  855.],\n",
       "         [ 205.,  430.,  675.,  735.,  795.,  855.]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = conv1d_layer(examples)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now that all that is established, let's look at having a variable length output for each timestep of the model (i.e. instead of just guessing one value per channel - the next value presumably - can it guess the next `p` time steps for each channel). So now we're opening to have an output that is 4 dimensional instead of 3.\n",
    "\n",
    "To back up a bit, here is what we expect from a multivariate output that is hoping to guess only the next data point (i.e. `p=1`) which for the first example looks like this (Note that we don't know the future of the series so the input of [50,55] won't have a \"target\" value in training - because... how could you possibly guess what comes next in this pattern?!?!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "tensor([[[ 0., 10., 20., 30., 40., 50.],\n",
      "         [ 5., 15., 25., 35., 45., 55.]]])\n",
      "\n",
      "OUTPUT:\n",
      "tensor([[[10., 20., 30., 40., 50.],\n",
      "         [15., 25., 35., 45., 55.]]])\n"
     ]
    }
   ],
   "source": [
    "print(f'INPUT:\\n{example1}\\n')\n",
    "\n",
    "print(f'OUTPUT:\\n{example1[:,:,1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want the output of at the first time step for the two channels to be [10,15] when given the history leading up to and including the [0,5] current time step input (again remember we have a bunch of zeros padded in front of the input time dimension so that we can get a prediction corresponding to every input timestep).  \n",
    "\n",
    "That's the paradigm we've been working with thus far, but what if we wanted to give the model the ability to guess the next p timesteps at each point. This means the model would have to be a lot smarter because it would need to guess farther into the future as well as the underlying patterns of the dataset (think underlying trend or frequency response of the data).  So instead of our output being the above, it would be some sort of rolling window like this (note that this is how I create `y_true` in the final model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "tensor([[[ 0., 10., 20., 30., 40., 50.],\n",
      "         [ 5., 15., 25., 35., 45., 55.]]])\n",
      "\n",
      "OUTPUT:\n",
      "tensor([[[[10., 20., 30.],\n",
      "          [20., 30., 40.],\n",
      "          [30., 40., 50.]],\n",
      "\n",
      "         [[15., 25., 35.],\n",
      "          [25., 35., 45.],\n",
      "          [35., 45., 55.]]]])\n"
     ]
    }
   ],
   "source": [
    "def torch_running_view(tensor,window,axis=-1):\n",
    "    shape = list(tensor.shape)\n",
    "    shape[axis] -= (window -1)\n",
    "    assert(shape[axis]>0)\n",
    "    return torch.as_strided(tensor, shape + [window], \n",
    "                            tensor.stride() + (tensor.stride()[axis],))\n",
    "\n",
    "print(f'INPUT:\\n{example1}\\n')\n",
    "\n",
    "out_timesteps = 3\n",
    "rolling_output = torch_running_view(example1, out_timesteps)\n",
    "print(f'OUTPUT:\\n{rolling_output[:,:,1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the expected dimensions of `torch.nn.Conv1d` are `batch_size, out_channels, seq_len`, the dimensions of the above correspond with `batch_size, out_channels, seq_len, out_timesteps` (so `[:,:,1:]` still cuts the first value of the sequence forcing the model to guess \"next\" not the current state). For example, starting with [0,5] and padding, we want the first output of channel 1 to be [10, 20, 30] (it's contribution was the 0) and the output of channel 2 to be [15, 25, 35] (it's contribution was the 5). Notice instead of `Tin - 1` like before, how output sequence is of length `Tin - p` (where T is seq_len of input and p is number of expected output timesteps at each step).  Thus, we might as well not even feed in the last p of the input sequence examples into the model because they cannot be scored.\n",
    "\n",
    "So how do we make our `CausalConv1d` class give us a 4th dimension when it naturally wants to give us 3?  Easy, we just make it guess extra channels and then reshape the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "dilation = 1\n",
    "stride = 1\n",
    "in_channels = 2\n",
    "out_channels = in_channels # autoencoder remember\n",
    "# new\n",
    "out_timesteps = 3\n",
    "\n",
    "multi_output_conv1d_layer = Causal_1dConv(in_channels = in_channels, \n",
    "                                         # new\n",
    "                                         out_channels = out_channels * out_timesteps,\n",
    "                                         kernel_size = kernel_size, \n",
    "                                         bias = False,\n",
    "                                         dilation = dilation,\n",
    "                                        stride = stride)\n",
    "\n",
    "# make weights be an averaging function or simple sum\n",
    "multi_output_conv1d_layer.conv.weight.data = torch.ones(out_channels * out_timesteps,in_channels,kernel_size)\n",
    "multi_output_conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the 2nd kernel be 2's instead of 1's to get a sense as to how the output changes (as seen above we have `out_channels * out_timesteps` number of `in_channels,kernel_size` shaped kernels that in a practical situation will all be different and create different output channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_output_conv1d_layer.conv.weight.data[1] = 2\n",
    "multi_output_conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to remind us what example1 looks like with a little padding (i.e. what the model will see on the forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "tensor([[[ 0., 10., 20., 30., 40., 50.],\n",
      "         [ 5., 15., 25., 35., 45., 55.]]])\n",
      "\n",
      "Padded Input:\n",
      "tensor([[[ 0.,  0.,  0., 10., 20., 30., 40., 50.],\n",
      "         [ 0.,  0.,  5., 15., 25., 35., 45., 55.]]])\n"
     ]
    }
   ],
   "source": [
    "signal = example1\n",
    "print(f'Original:\\n{signal}\\n')\n",
    "padding = (int((kernel_size - 1) * (dilation)), 0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "print(f'Padded Input:\\n{signal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we convolve the padded input with those 6 kernels, we get 6 output series that are all the same length (`Tin`) as the input series.  We can also so that the second kernel predictably has created an output channel with values that are double in magnitude of the others (again, the kernels that are simply 1's are essentially just adding numbers from the input together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.,  30.,  75., 135., 195., 255.],\n",
       "         [ 10.,  60., 150., 270., 390., 510.],\n",
       "         [  5.,  30.,  75., 135., 195., 255.],\n",
       "         [  5.,  30.,  75., 135., 195., 255.],\n",
       "         [  5.,  30.,  75., 135., 195., 255.],\n",
       "         [  5.,  30.,  75., 135., 195., 255.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_multi_output = multi_output_conv1d_layer(example1)\n",
    "raw_multi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.,  30.,  75.],\n",
       "         [ 10.,  60., 150.],\n",
       "         [  5.,  30.,  75.],\n",
       "         [  5.,  30.,  75.],\n",
       "         [  5.,  30.,  75.],\n",
       "         [  5.,  30.,  75.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = example1.shape[-1]\n",
    "p = out_timesteps\n",
    "Tin = T - p\n",
    "a = multi_output_conv1d_layer(example1[:,:,:Tin])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we padded, the output sequences are all the same length as the input but we remember that we won't be able to score the last `p` predictions from each output channel.  We will remove them after reshaping the output in two steps:\n",
    "\n",
    "First, we split the final dimension into `(out_timesteps, seq_len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,  30.,  75., 135., 195., 255.],\n",
       "          [ 10.,  60., 150., 270., 390., 510.],\n",
       "          [  5.,  30.,  75., 135., 195., 255.]],\n",
       "\n",
       "         [[  5.,  30.,  75., 135., 195., 255.],\n",
       "          [  5.,  30.,  75., 135., 195., 255.],\n",
       "          [  5.,  30.,  75., 135., 195., 255.]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin = example1.shape[-1]  # length of input series\n",
    "modified_output = raw_multi_output.view(1,in_channels,out_timesteps,Tin)\n",
    "modified_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically thats sufficient but I think it's weird to have the dimensions be `batch_size, out_channels, out_timesteps, seq_len` so I flip the last two dimensions (this also lines it up with the output that I showed above with the rolling window which is the function I use to create the `y_true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,  10.,   5.],\n",
       "          [ 30.,  60.,  30.],\n",
       "          [ 75., 150.,  75.],\n",
       "          [135., 270., 135.],\n",
       "          [195., 390., 195.],\n",
       "          [255., 510., 255.]],\n",
       "\n",
       "         [[  5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.],\n",
       "          [ 75.,  75.,  75.],\n",
       "          [135., 135., 135.],\n",
       "          [195., 195., 195.],\n",
       "          [255., 255., 255.]]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_output = modified_output.permute((0,1,3,2)) # flipping 2nd and 3rd dims\n",
    "print(reshaped_output.shape)\n",
    "reshaped_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that we don't have future values for all the timesteps in the input series. Since we can't score them, let's remove the last p from the `seq_len` dim (the last one now). This will actually be included by reshaping the inputs but I wanted to point it out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,  10.,   5.],\n",
       "          [ 30.,  60.,  30.],\n",
       "          [ 75., 150.,  75.]],\n",
       "\n",
       "         [[  5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.],\n",
       "          [ 75.,  75.,  75.]]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of output series (not actually used but included to provide further context)\n",
    "Tout = Tin - out_timesteps \n",
    "final_output = reshaped_output[:,:,:out_timesteps,:]\n",
    "print(final_output.shape)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok sanity check, those two clusters above represent the output of two channels.  Let's look at the second channel because none of its values were generated with the 2's kernel. Thus, at each time step, the model generates a string of 3 equivalent values (first: 5's, second: 30s, and finally: 75s).  That should pass our sanity check because each kernel is identical which means that the outputs should be identical if it's seeing the same inputs (if you want to check this, remember that the values the kernel sees are simply added when the kernel is all 1's)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together in a class that would allow us to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 1\n",
    "dilation = 1\n",
    "stride = 3\n",
    "in_channels = 2\n",
    "out_channels = in_channels # autoencoder remember\n",
    "# new\n",
    "out_timesteps = 3\n",
    "\n",
    "test1 = Causal_1dConv(in_channels = in_channels, \n",
    "                                         # new\n",
    "                                         out_channels = out_channels * out_timesteps,\n",
    "                                         kernel_size = kernel_size, \n",
    "                                         bias = False,\n",
    "                                         dilation = dilation,\n",
    "                                        stride = 1)\n",
    "\n",
    "test2 = Causal_1dConv(in_channels = in_channels, \n",
    "                                         # new\n",
    "                                         out_channels = out_channels * out_timesteps,\n",
    "                                         kernel_size = kernel_size, \n",
    "                                         bias = False,\n",
    "                                         dilation = dilation,\n",
    "                                        stride = stride)\n",
    "\n",
    "# make weights be an averaging function or simple sum\n",
    "test1.conv.weight.data = torch.ones(out_channels * out_timesteps,in_channels,kernel_size)\n",
    "test2.conv.weight.data = torch.ones(out_channels * out_timesteps,in_channels,kernel_size)\n",
    "# test.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Example:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.,\n",
      "          110., 120., 130.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.,\n",
      "          115., 125., 135.]]])\n",
      "torch.Size([1, 2, 14])\n",
      "\n",
      "tensor([[  5.,  65., 125., 185.]], grad_fn=<SelectBackward>) \n",
      " torch.Size([1, 6, 4])\n",
      "tensor([[  5.,  65., 125., 185.]], grad_fn=<SelectBackward>) \n",
      " torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "seq_len =14\n",
    "channels = 2\n",
    "\n",
    "example3 = torch.zeros((1,channels,seq_len)) + torch.arange(seq_len).float() * 10\n",
    "example3[:,1] = example3[:,1] + 5\n",
    "print(f'1st Example:\\n{example3}\\n{example3.shape}\\n')\n",
    "\n",
    "o1 = test1(example3[:,:,:-out_timesteps])[:,:,::stride]\n",
    "o2 = test2(example3[:,:,:-out_timesteps])\n",
    "print(o1[:,0],'\\n',o1.shape)\n",
    "print(o2[:,0],'\\n',o2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Series:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.,\n",
      "          110., 120., 130.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.,\n",
      "          115., 125., 135.]]])\n",
      "torch.Size([1, 2, 14])\n",
      "\n",
      "X:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.]]])\n",
      "torch.Size([1, 2, 11])\n",
      "\n",
      "preds:\n",
      "tensor([[[[  5.,   5.,   5.],\n",
      "          [ 65.,  65.,  65.],\n",
      "          [125., 125., 125.],\n",
      "          [185., 185., 185.]],\n",
      "\n",
      "         [[  5.,   5.,   5.],\n",
      "          [ 65.,  65.,  65.],\n",
      "          [125., 125., 125.],\n",
      "          [185., 185., 185.]]]], grad_fn=<PermuteBackward>)\n",
      "torch.Size([1, 2, 4, 3])\n",
      "\n",
      "targets:\n",
      "tensor([[[[ 10.,  20.,  30.],\n",
      "          [ 40.,  50.,  60.],\n",
      "          [ 70.,  80.,  90.],\n",
      "          [100., 110., 120.]],\n",
      "\n",
      "         [[ 15.,  25.,  35.],\n",
      "          [ 45.,  55.,  65.],\n",
      "          [ 75.,  85.,  95.],\n",
      "          [105., 115., 125.]]]])\n",
      "torch.Size([1, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "def shape_inputs(signal,out_timesteps,kernel_size,dilation):\n",
    "    # pad so that it's causal\n",
    "    padding = (int((kernel_size - 1) * (dilation)), 0)\n",
    "    X = torch.nn.functional.pad(signal, padding)\n",
    "    \n",
    "    # remove timesteps that we won't have predictions for\n",
    "    X = X[:,:,:-out_timesteps]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def shape_preds(preds, X, out_timesteps):\n",
    "    Tin = X.shape[-1]\n",
    "    in_channels = X.shape[1]\n",
    "    preds = preds.view(1,in_channels,out_timesteps,-1)\n",
    "    preds = preds.permute((0,1,3,2))\n",
    "    return preds\n",
    "\n",
    "def shape_targets(signal,out_timesteps,stride):\n",
    "    future_vals = signal[:,:,1:] # all but the first\n",
    "    targets = torch_running_view(future_vals,out_timesteps) # reshape to have a 4th dimension\n",
    "    targets = targets[:,:,::stride] # sample according to striding\n",
    "    return targets\n",
    "\n",
    "X = shape_inputs(example3,out_timesteps,kernel_size,dilation)\n",
    "preds = test2(X)\n",
    "preds = shape_preds(preds, X, out_timesteps)\n",
    "targets = shape_targets(example3,out_timesteps,stride)\n",
    "\n",
    "print(f'Input Series:\\n{example3}\\n{example3.shape}\\n')\n",
    "print(f'X:\\n{X}\\n{X.shape}\\n')\n",
    "print(f'preds:\\n{preds}\\n{preds.shape}\\n')\n",
    "print(f'targets:\\n{targets}\\n{targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dAutoencoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A causal implementation of Conv1d\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, out_timesteps = 1,\n",
    "                 kernel_size=1, stride=1, dilation=1, bias=True):\n",
    "        super(Conv1dAutoencoder, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.out_timesteps = out_timesteps\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels= in_channels, \n",
    "                                    out_channels = out_channels * out_timesteps,\n",
    "                                    kernel_size=kernel_size, \n",
    "                                    stride=stride,\n",
    "                                    dilation=dilation, \n",
    "                                    bias=bias)\n",
    "\n",
    "    def forward(self, signal):\n",
    "        X = self.shape_inputs(signal)\n",
    "        \n",
    "        targets = self.shape_targets(signal)\n",
    "\n",
    "        preds = self.conv(X)\n",
    "        preds = self.reshape_preds(preds)\n",
    "\n",
    "        return X, preds, targets\n",
    "    \n",
    "    def shape_inputs(self,signal):\n",
    "        # pad so that it's causal\n",
    "        padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
    "        X = torch.nn.functional.pad(signal, padding)\n",
    "        # remove timesteps that we won't have predictions for\n",
    "        X = X[:,:,:-self.out_timesteps]\n",
    "        return X  \n",
    "    \n",
    "    def reshape_preds(self,preds):\n",
    "        preds = preds.view(1,self.in_channels,self.out_timesteps,-1)\n",
    "        preds = preds.permute((0,1,3,2))\n",
    "        return preds\n",
    "    \n",
    "    def shape_targets(self,signal):\n",
    "        future_vals = signal[:,:,1:] # all but the first\n",
    "        targets = torch_running_view(future_vals,self.out_timesteps) # reshape to have a 4th dimension\n",
    "        targets = targets[:,:,::self.stride] # sample according to striding\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th Example:\n",
      "tensor([[[ 0., 10., 20., 30., 40., 50., 60., 70., 80., 90.],\n",
      "         [ 5., 15., 25., 35., 45., 55., 65., 75., 85., 95.]]])\n",
      "torch.Size([1, 2, 10])\n",
      "\n",
      "X:\n",
      "tensor([[[ 0.,  0.,  0., 10., 20., 30., 40., 50., 60.],\n",
      "         [ 0.,  0.,  5., 15., 25., 35., 45., 55., 65.]]])\n",
      "torch.Size([1, 2, 9])\n",
      "\n",
      "preds:\n",
      "tensor([[[[  5.,   5.,   5.],\n",
      "          [ 90.,  90.,  90.],\n",
      "          [210., 210., 210.]],\n",
      "\n",
      "         [[  5.,   5.,   5.],\n",
      "          [ 90.,  90.,  90.],\n",
      "          [210., 210., 210.]]]], grad_fn=<PermuteBackward>)\n",
      "torch.Size([1, 2, 3, 3])\n",
      "\n",
      "targets:\n",
      "tensor([[[[10., 20., 30.],\n",
      "          [40., 50., 60.],\n",
      "          [70., 80., 90.]],\n",
      "\n",
      "         [[15., 25., 35.],\n",
      "          [45., 55., 65.],\n",
      "          [75., 85., 95.]]]])\n",
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 2\n",
    "dilation = 2\n",
    "stride = 3\n",
    "in_channels = 2\n",
    "out_channels = in_channels # autoencoder remember\n",
    "# new\n",
    "out_timesteps = 3\n",
    "\n",
    "model = Conv1dAutoencoder(in_channels = in_channels, \n",
    "                         # new\n",
    "                         out_channels = out_channels,\n",
    "                          out_timesteps = out_timesteps,\n",
    "                         kernel_size = kernel_size, \n",
    "                         bias = False,\n",
    "                         dilation = dilation,\n",
    "                         stride = stride)\n",
    "\n",
    "model.conv.weight.data = torch.ones(out_channels * out_timesteps,in_channels,kernel_size)\n",
    "\n",
    "seq_len = 10\n",
    "example4 = torch.zeros((1,2,seq_len)) + torch.arange(seq_len).float() * 10\n",
    "example4[:,1] = example4[:,1] + 5\n",
    "print(f'4th Example:\\n{example4}\\n{example4.shape}\\n')\n",
    "\n",
    "X, preds, targets = model(example4)\n",
    "\n",
    "\n",
    "\n",
    "print(f'X:\\n{X}\\n{X.shape}\\n')\n",
    "print(f'preds:\\n{preds}\\n{preds.shape}\\n')\n",
    "print(f'targets:\\n{targets}\\n{targets.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can break it... doesn't seem like it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th Example:\n",
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.,\n",
      "          110., 120., 130., 140., 150., 160., 170., 180., 190., 200., 210.,\n",
      "          220., 230., 240., 250., 260., 270., 280., 290.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.,\n",
      "          115., 125., 135., 145., 155., 165., 175., 185., 195., 205., 215.,\n",
      "          225., 235., 245., 255., 265., 275., 285., 295.]]])\n",
      "torch.Size([1, 2, 30])\n",
      "\n",
      "X:\n",
      "tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90.,\n",
      "          100., 110., 120., 130., 140., 150., 160., 170., 180., 190.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95.,\n",
      "          105., 115., 125., 135., 145., 155., 165., 175., 185., 195.]]])\n",
      "torch.Size([1, 2, 32])\n",
      "\n",
      "preds:\n",
      "tensor([[[[   5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.],\n",
      "          [ 110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.],\n",
      "          [ 315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.],\n",
      "          [ 625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.],\n",
      "          [1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025.]],\n",
      "\n",
      "         [[   5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.,    5.],\n",
      "          [ 110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.,  110.],\n",
      "          [ 315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.,  315.],\n",
      "          [ 625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.,  625.],\n",
      "          [1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025., 1025.]]]],\n",
      "       grad_fn=<PermuteBackward>)\n",
      "torch.Size([1, 2, 5, 10])\n",
      "\n",
      "targets:\n",
      "tensor([[[[ 10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.],\n",
      "          [ 50.,  60.,  70.,  80.,  90., 100., 110., 120., 130., 140.],\n",
      "          [ 90., 100., 110., 120., 130., 140., 150., 160., 170., 180.],\n",
      "          [130., 140., 150., 160., 170., 180., 190., 200., 210., 220.],\n",
      "          [170., 180., 190., 200., 210., 220., 230., 240., 250., 260.]],\n",
      "\n",
      "         [[ 15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.],\n",
      "          [ 55.,  65.,  75.,  85.,  95., 105., 115., 125., 135., 145.],\n",
      "          [ 95., 105., 115., 125., 135., 145., 155., 165., 175., 185.],\n",
      "          [135., 145., 155., 165., 175., 185., 195., 205., 215., 225.],\n",
      "          [175., 185., 195., 205., 215., 225., 235., 245., 255., 265.]]]])\n",
      "torch.Size([1, 2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "dilation = 3\n",
    "stride = 4\n",
    "in_channels = 2\n",
    "out_channels = in_channels # autoencoder remember\n",
    "# new\n",
    "out_timesteps = 10\n",
    "\n",
    "model = Conv1dAutoencoder(in_channels = in_channels, \n",
    "                         # new\n",
    "                         out_channels = out_channels,\n",
    "                          out_timesteps = out_timesteps,\n",
    "                         kernel_size = kernel_size, \n",
    "                         bias = False,\n",
    "                         dilation = dilation,\n",
    "                         stride = stride)\n",
    "\n",
    "model.conv.weight.data = torch.ones(out_channels * out_timesteps,in_channels,kernel_size)\n",
    "\n",
    "seq_len = 30\n",
    "example4 = torch.zeros((1,2,seq_len)) + torch.arange(seq_len).float() * 10\n",
    "example4[:,1] = example4[:,1] + 5\n",
    "print(f'4th Example:\\n{example4}\\n{example4.shape}\\n')\n",
    "\n",
    "X, preds, targets = model(example4)\n",
    "\n",
    "\n",
    "\n",
    "print(f'X:\\n{X}\\n{X.shape}\\n')\n",
    "print(f'preds:\\n{preds}\\n{preds.shape}\\n')\n",
    "print(f'targets:\\n{targets}\\n{targets.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self, in_channels, out_channels, output_len,\n",
    "                 kernel_size=1, stride=1,dilation=1):\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_len = output_len\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        \n",
    "        self.model = Causal_1dConv(in_channels = in_channels,\n",
    "                                   out_channels = out_channels * output_len,\n",
    "                                   kernel_size = kernel_size, \n",
    "                                   bias = False, stride = stride,\n",
    "                                   dilation = dilation)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        assert x.shape[1] == self.in_channels, 'not proper number of channels'\n",
    "        seq_len = x.shape[2]\n",
    "        \n",
    "        # should be 3 dim\n",
    "        preds = self.model(x)\n",
    "        print(preds.shape)\n",
    "        # needs to be expanded to 4 in order to overlay on top of y_true\n",
    "        # batch_size, channels, seq_len, output_len\n",
    "# uncomment this\n",
    "        #         preds = preds.view(batch_size,self.out_channels,-1,self.output_len) \n",
    "# UNCOMMENT THIS\n",
    "        \n",
    "        # padding is independent of dilation \n",
    "        padding = (int(self.kernel_size - 1 ), 0)\n",
    "        x = torch.nn.functional.pad(x, padding)\n",
    "        y_true = torch_running_view(x,self.output_len)\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150., 160., 170.],\n",
      "         [105., 115., 125., 135., 145., 155., 165., 175.]]])\n",
      "shape:    torch.Size([2, 2, 8])\n",
      "torch.Size([1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 8]), torch.Size([1, 2, 6, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "print(data)\n",
    "print('shape:   ',data.shape)\n",
    "\n",
    "\n",
    "chans =  data.shape[1]\n",
    "kernel_size = 2\n",
    "output_len = 4\n",
    "dilation = 1\n",
    "\n",
    "autoencoder = Autoencoder(in_channels = chans, stride=1,\n",
    "                          out_channels = chans,\n",
    "                          output_len = output_len,\n",
    "                          kernel_size = kernel_size,\n",
    "                         dilation=dilation)\n",
    "\n",
    "\n",
    "autoencoder.model.conv.weight.data = torch.ones(chans*output_len,chans,kernel_size)\n",
    "# autoencoder.model.conv.weight.data[1,:,:] = 2\n",
    "preds, y_true = autoencoder.predict(data[0].view(1,2,-1))\n",
    "\n",
    "preds.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.model.conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.model.conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 0.,  5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = data[0]\n",
    "padding = (int((kernel_size-1)*dilation),0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f25951b4356d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# batch, chans, seqlen, output_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "# batch, chans, seqlen, output_len\n",
    "preds[0,0,:,:]#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# its repeating information because everything is a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.]],\n",
       "\n",
       "         [[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.]]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an extra dim to split the channels from output dim (remember we're having the conv layer give us num_chan * output_dim)\n",
    "test = preds.view(1,chans,output_len,data.shape[-1])#.permute((0,1,3,2))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 5., 5., 5.], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch, chan, output_index, current time\n",
    "test[0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,   5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.,  30.],\n",
       "          [ 70.,  70.,  70.,  70.],\n",
       "          [110., 110., 110., 110.],\n",
       "          [150., 150., 150., 150.],\n",
       "          [190., 190., 190., 190.],\n",
       "          [230., 230., 230., 230.],\n",
       "          [270., 270., 270., 270.]],\n",
       "\n",
       "         [[  5.,   5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.,  30.],\n",
       "          [ 70.,  70.,  70.,  70.],\n",
       "          [110., 110., 110., 110.],\n",
       "          [150., 150., 150., 150.],\n",
       "          [190., 190., 190., 190.],\n",
       "          [230., 230., 230., 230.],\n",
       "          [270., 270., 270., 270.]]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to match the shape of y_true we need to swap the axis (using permute) \n",
    "# each row is all the same because all four of the kernels (or 8 if you consider both channels) are seeing\n",
    "# the same input at that time step and all the kernels are also identical\n",
    "# ideally the output kernels would learn to be different based on what the input is (which right now is not only 2 time steps of two channels\n",
    "# so not a whole lot of information for it to expand to 8 values)\n",
    "\n",
    "# I guess if we had multiple layers, only the output would need to receive this reshaping treatment\n",
    "test2 = test.permute((0,1,3,2))\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 0.,  5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = data[0]\n",
    "padding = (int((kernel_size-1)*dilation),0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0., 10., 20.],\n",
       "          [ 0., 10., 20., 30.],\n",
       "          [10., 20., 30., 40.],\n",
       "          [20., 30., 40., 50.],\n",
       "          [30., 40., 50., 60.],\n",
       "          [40., 50., 60., 70.]],\n",
       "\n",
       "         [[ 0.,  5., 15., 25.],\n",
       "          [ 5., 15., 25., 35.],\n",
       "          [15., 25., 35., 45.],\n",
       "          [25., 35., 45., 55.],\n",
       "          [35., 45., 55., 65.],\n",
       "          [45., 55., 65., 75.]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d758a95b0509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "(batch_size,self.out_channels,-1,self.output_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-4, 3], but got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-22c698e32fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-4, 3], but got 4)"
     ]
    }
   ],
   "source": [
    "preds.view(1,2,data.shape[-1],-1).transpose(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat((torch.ones(1, 24, 24),\n",
    "               torch.ones(1, 24, 24)*2,\n",
    "               torch.ones(1, 24, 24)*3), 0)\n",
    "\n",
    "permute = [2, 1, 0]\n",
    "x = x[permute, :, :]\n",
    "y = x.numpy()\n",
    "c, h, w = y.shape\n",
    "y = y.transpose(1, 2, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         ...,\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.]],\n",
       "\n",
       "        [[2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         ...,\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0., 10.],\n",
       "          [10., 20.],\n",
       "          [20., 30.],\n",
       "          [30., 40.],\n",
       "          [40., 50.],\n",
       "          [50., 60.],\n",
       "          [60., 70.]],\n",
       "\n",
       "         [[ 0.,  5.],\n",
       "          [ 5., 15.],\n",
       "          [15., 25.],\n",
       "          [25., 35.],\n",
       "          [35., 45.],\n",
       "          [45., 55.],\n",
       "          [55., 65.],\n",
       "          [65., 75.]]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150., 160., 170.],\n",
      "         [105., 115., 125., 135., 145., 155., 165., 175.]]])\n",
      "shape:    torch.Size([2, 2, 8])\n",
      "reshaped: torch.Size([2, 2, 6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10., 20.],\n",
       "        [10., 20., 30.],\n",
       "        [20., 30., 40.],\n",
       "        [30., 40., 50.],\n",
       "        [40., 50., 60.],\n",
       "        [50., 60., 70.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def torch_running_view(tensor,window,axis=-1):\n",
    "    shape = list(tensor.shape)\n",
    "    shape[axis] -= (window -1)\n",
    "    assert(shape[axis]>0)\n",
    "    return torch.as_strided(tensor, shape + [window], \n",
    "                            tensor.stride() + (tensor.stride()[axis],))\n",
    "\n",
    "# data = torch.arange(10).view(2,1,-1)\n",
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "print(data)\n",
    "print('shape:   ',data.shape)\n",
    "\n",
    "reshaped_data = torch_running_view(data,3)\n",
    "print(\"reshaped:\",reshaped_data.shape)\n",
    "reshaped_data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([ 0.,  1.,  3.,  6., 10., 15., 20., 25., 30., 35., 40.],\n",
      "       grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(20).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(20).float()\n",
    "\n",
    "data *10 +5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100., 110., 120., 130., 140., 150., 160., 170.],\n",
       "        [105., 115., 125., 135., 145., 155., 165., 175.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "as_strided(): argument 'stride' (position 3) must be tuple of ints, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2385af3d5c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_strided\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: as_strided(): argument 'stride' (position 3) must be tuple of ints, not int"
     ]
    }
   ],
   "source": [
    "torch.as_strided(data[0,0],(2,-1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_view(arr, window, axis=-1):\n",
    "    \"\"\"\n",
    "    return a running view of length 'window' over 'axis'\n",
    "    the returned array has an extra last dimension, which spans the window\n",
    "    \"\"\"\n",
    "    shape = list(arr.shape)\n",
    "    shape[axis] -= (window-1)\n",
    "    assert(shape[axis]>0)\n",
    "    return np.lib.index_tricks.as_strided(\n",
    "        arr,\n",
    "        shape + [window],\n",
    "        arr.strides + (arr.strides[axis],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_rolling_window(x, window_size, step_size=1):\n",
    "    # unfold dimension to make our rolling window\n",
    "    return x.unfold(0,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 5],\n",
       "         [1, 6],\n",
       "         [2, 7],\n",
       "         [3, 8],\n",
       "         [4, 9]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(10).view(2,-1)\n",
    "\n",
    "pytorch_rolling_window(data,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.arange(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4],\n",
       "       [4, 5],\n",
       "       [5, 6],\n",
       "       [6, 7],\n",
       "       [7, 8],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_view(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to CNN (must read if need clarification on some of the terminology) https://arxiv.org/pdf/1603.07285.pdf\n",
    "\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last decade, we witnessed great strides in the field of computer vision through the utilization and development of deep convolutional neural network architectures. Thus, much attention has been given to the development and manipulation of these convolutional models both in terms of theory and ease of implementation/prototyping (think PyTorch, TensorFlow, etc).\n",
    "\n",
    "However, time series modeling can also use this and is really can just be thought of as a simpler 1D version of the 2D.  We would like to  \n",
    "\n",
    "This tutorial will focus on providing a top down approach to building a 1D convolutional model in PyTorch.  We will walk through the arithmetic of the different hyperparameters of such models in order to provide a solid foundation for more sophisticated hypertuning of these parameters to build highly complex and powerful neural network architectures for time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's `nn.Conv1d` Class\n",
    "\n",
    "We will focus on a code-first, top-down approach and fill in theoretical details as needed. The hope is that by the end of this you will feel comfortable working with all the levers made available by a modern, open-source, machine learning libraries. Thus, we begin with noting that PyTorch provides a native class for a 1D convolutional layer:\n",
    "\n",
    "`torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "We begin with the required arguments (`in_channels`, `out_channels`, `kernel_size`) of the `torch.nn.Conv1d` class before moving on to the keyword arguments.  The documentation defines them in the following way:\n",
    "```\n",
    "in_channels (python:int)  Number of channels in the input image\n",
    "out_channels (python:int)  Number of channels produced by the convolution\n",
    "kernel_size (python:int or tuple)  Size of the convolving kernel\n",
    "```\n",
    "Notice the slight Freudian slip of the architects assuming that the input would be an \"image.\"  Since we're undyingly grateful for the work they've done, we'll let this slide and instead focus on discussing the implication of being able to specify the number of channels in a \"so-called\" 1D model. \n",
    "\n",
    "## The Univariate Problem: 1 channel in, 1 channel out\n",
    "\n",
    "Again, we're taking a build-first approach so let's just start with a simple example: 1 channel in, 1 channel out, and a kernel_size of 5.  This is a realistic starting point since, oftentimes, a time series problem is framed such that a univariate (single channel input) dataset is used to predict the future value of itself (single channel output).  (Aside: we will also remove the bias term from the model for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 1, kernel_size=(5,), stride=(1,), bias=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = torch.nn.Conv1d(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False)\n",
    "conv1d_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we've just created our first 1D convolutional layer that inherently leverages all the power of PyTorch's autograd functionality, GPU utilization, etc.  But... what was actually created? Well let's take a look at the kernel (or weight matrix if you prefer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.0071,  0.1819,  0.4029, -0.4130,  0.0206]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, PyTorch has randomly generated 5 numbers to act as our initial weight tensor.  However, if you'd like to use your own weight initialization logic you can overlay this by accessing the values of the tensor using the `data` attribute.  Let's say we'd previously optimized this model and knew that it should simply take an unweighted moving average of the last 5 data points. We would modify all the weights to be 1/5 like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use similar logic to overlay any number of weight initialization protocols.\n",
    "\n",
    "OK, so we have a model (which takes an unweighted average over the 5 most recent data points), let's create a fake stream of time series data and pass it in to see what happens (of course, reshaping to the expected dimensions of `[batch_size, channels_in, seq_length]` and make sure that the type is `float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 14])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[5.2000, 5.8000, 5.8000, 5.6000, 5.6000, 5.8000, 5.6000, 5.4000,\n",
       "          6.8000, 5.8000]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([4, 9, 3, 6, 4, 7, 9, 2, 6, 5, 6, 8, 9, 1, 4]).float()\n",
    "# reformat data [batch, channels, seq_len]\n",
    "X = data.view(1,1,-1)[:,:,:-1]\n",
    "print(X.shape)\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "print(output.squeeze().shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY ISN'T IT CAUSAL? Seems like it is if set up like this\n",
    "\n",
    "the average of [0., 1., 2., 3., 4] is 2 and likewise is 3 of [1., 2., 3., 4, 5].  So clearly its taking the first 5 then shifting 1 to the right...etc.  I can set y_true to be 5 and 6 and let it modify it's weights but to me this proves that it's doing what we hope it would unless I'm missing something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([0., 1., 2., 3., 4., 5.])\n",
      "y_true: tensor([5., 6.])\n",
      "y_pred: tensor([2., 3.], grad_fn=<RoundBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(7).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)[:,:,:-1]\n",
    "y_true = data.squeeze()[kernel_size:]\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {torch.round(output.view(-1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = torch.nn.Conv1d(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False,dilation=2)\n",
    "conv1d_layer.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([0.0000, 0.2000, 0.6000, 1.2000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000,\n",
      "        7.0000, 8.0000], grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(11).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wavenets implementation is easier, maybe don't introduce it until later\n",
    "\n",
    "https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py#L23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolution with the option to be causal and use xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 dilation=1, bias=True, w_init_gain='linear', is_causal=False):\n",
    "        super(Conv, self).__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    dilation=dilation, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        if self.is_causal:\n",
    "                padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
    "                signal = torch.nn.functional.pad(signal, padding) \n",
    "        return self.conv(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfronczak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = Conv(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False,dilation=1,is_causal=True)\n",
    "conv1d_layer.conv.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([0.0000, 0.2000, 0.6000, 1.2000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000,\n",
      "        7.0000, 8.0000], grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(11).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this guy goes into how to restructuredata if you have multivariate, stacked output https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0,2,4,6,8])+1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh the simplicity of PyTorch.  All we needed to do was pass our input data in as an argument to the model we created and it generates the output series based on the weights stored (or optimized as that may be the case).  But you may notice that there are only 11 values in the output series whereas our original dataset had 15.  Recall that we are taking a moving average of 5 data points which means that, in order to make an inference at a particular point in time, we need to know the previous 4 values.  Thus, our first output can only come once we have \"seen\" 5 data points. The figure provides a visual of how a 1d convolutional kernel slides along the time axis of our input series to provide a single output value at every timestep after skipping the first 4.  Again, PyTorch handles all of this logic internally for us but understanding the expected dimensional output is still very important for model scoring, prototyping, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Picture of sliding conv on time axis\n",
    "\n",
    "11 small pictures with shading to show where kernel is focusing, and then one of them will be a zoomed in to show the math\n",
    "\n",
    "input\n",
    "|\n",
    "kernel\n",
    "|\n",
    "output\n",
    "\n",
    "note the directionality of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multivariate Problem: `n` input channels, `m` output channels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For example, let's say that we were interest in predicting the future value of a particular stock and only wanted to use past-values of that stock to do so.  That would mean that both `in_channels` and `out_channels` would be set to 1.  However, if we wanted to use the 5 most similar company's stock prices to predict the value of a single stock then `out_channels` would stay \n",
    "\n",
    "Ok enough talk, let's start building...\n",
    "\n",
    "the input of a 1D model would ever have more  why a time series would ever be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The bread and butter of neural networks is affine transformations: a vector\n",
    "is received as input and is multiplied with a matrix to produce an output (to\n",
    "which a bias vector is usually added before passing the result through a nonlinearity).\" https://arxiv.org/pdf/1603.07285.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic 1d Conv\n",
    "\n",
    "Look at what the basic output looks like\n",
    "\n",
    "1 channel in and 1 channel out, and a kernel length of 5,\n",
    "\n",
    "no extra striding (stride =1), no extra padding (paadding = 0), no dilation (dilation =1)\n",
    "\n",
    "note that we are only required to give `in_channels, out_channels, kernel_size`. left the others there with their defaults (except bias, we're turning that off for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2755,  0.1295, -0.0642,  0.2511,  0.3457]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =5,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some simple data, 2 channels with sequence length: 15 (obviously not very interesting data here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones((1,15))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets grab the first channel and add 2 dummy dimensions to it since that's what's expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = data[0].view(1,1,-1)\n",
    "# inp[0,0,-1] = 4\n",
    "# inp[0,0,4] = 4\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it into the model and get 11 numbers out which makes sense because the first four inputs didn't have enough past history to make a prediction (kernel size = 5) so it's only at the 5th element that we get our first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376,\n",
       "          0.9376, 0.9376, 0.9376]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the value, it's not surprisingly constant but its also just the sum of all the elements in the kernel since our data is simply ones ($\\sum_{i=0}^k w_i$ where $k$ is kernal length and $w_i$ refers to each weight-element in the kernel). \n",
    "\n",
    "One can access the kernel weights with the `weight` attribute for the layer in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6312, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### kernel is not a multiple of sequence length\n",
    "\n",
    "What happens if we change the kernal length to be a non-multiple of the sequence length (k = 4 and seq_len = 15)\n",
    "\n",
    "Not a problem since our stride is 1 and we don't have any padding, it just follows the same pattern above where the first k-1 points don't recieve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.4973, -0.2145,  0.1587, -0.0901]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =4,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again since the kernel is overlaid on top of the sequence, the first 3 values in the sequence don't have corresponding outputs from the model (since it is a requirement to have 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6433, -0.6433, -0.6433, -0.6433, -0.6433, -0.6433, -0.6433,\n",
       "          -0.6433, -0.6433, -0.6433, -0.6433, -0.6433]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inputs\n",
    "\n",
    "So what happens when we just change the number of input channels but still expect 1 channel in the output. We see that the weight tensor now has 2 kernels of length 5 (so the shape is (1,2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones((2,15))\n",
    "data[1] += 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 5.5815e-02,  6.2070e-02,  1.1655e-01, -1.6826e-01, -2.7581e-01],\n",
      "         [ 2.0035e-01, -1.8993e-04,  1.6400e-01,  4.2376e-02,  1.4429e-01]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 2, out_channels = 1, kernel_size =5,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 2 dummy dimension to the tensor since that's what's expected by torch (shape = (1,2,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = data.view(1,2,15)\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920,\n",
       "          0.8920, 0.8920, 0.8920]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are constant again (as to be expected) and simply the sum of kernel1 convolving the first series (of ones) and kernel2 convolving the second series (of 2s).  Again, the first convolution is just the sum of the weight elements in the kernel ($\\sum_{i=0}^k w_i$) whereas the the second\n",
    "\n",
    "Note that it's just $\\sum_{i=0}^k 1*w_i$ where $k$ is kernal length and $w_i$ refers to each weight. It's mult by 1 right now since our series is just ones [1,1,1....,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8920, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel1 = model.weight[0,0]\n",
    "kernel2 = model.weight[0,1]\n",
    "\n",
    "kernel1.sum() * 1 + kernel2.sum() * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
       "          2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(1,1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride is how many steps to take before making a new calculation (if you say 0 it will just stay in one spot and obviously kill the kernel you're working on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.4267,  0.4077, -0.1108,  0.2961,  0.1737]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8097, 0.8097, 0.8097, 0.8097, 0.8097, 0.6989, 0.6989, 0.6989,\n",
       "          0.6989, 0.6989]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =5,\n",
    "                stride=1, padding=0, dilation=2, groups=1, \n",
    "                bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)\n",
    "model(a.view(1,1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4398, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight[0,0,1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0668, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight[].sum()*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3823)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0.2745, -0.1809,  0.0806, -0.3044, -0.2521]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2890)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0.2901,  0.1385, -0.2273, -0.1210,  0.2087]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19569999999999999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.3823 + 0.2890*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9473)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0807,  0.1279, -0.1774, -0.0982, -0.0345]).sum() + 2 * torch.tensor([-0.0052,  0.0016,  0.1569,  0.2386,  0.1325]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
