{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that to make it causal, we pad the front bit of the input before sending into the forward pass (see wavenet implementation https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py#L23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1., 1., 1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "dilation = 1\n",
    "\n",
    "conv1d_layer = Causal_1dConv(in_channels = 1, \n",
    "                             out_channels = 1,\n",
    "                             kernel_size = kernel_size, \n",
    "                             bias = False,\n",
    "                             dilation = dilation)\n",
    "\n",
    "# make weights be an averaging function or simple sum\n",
    "conv1d_layer.conv.weight.data = torch.ones(1,1,kernel_size) #/ kernel_size\n",
    "conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_1dConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolution with the option to be causal and use xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 dilation=1, bias=True, w_init_gain='linear', is_causal=True):\n",
    "        super(Causal_1dConv, self).__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    dilation=dilation, bias=bias)\n",
    "\n",
    "#         torch.nn.init.xavier_uniform(\n",
    "#             self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        if self.is_causal:\n",
    "                padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
    "                signal = torch.nn.functional.pad(signal, padding) \n",
    "        return self.conv(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = torch.arange(10)+5\n",
    "padding = (2,0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self, in_channels, out_channels, output_len,\n",
    "                 kernel_size=1, stride=1,dilation=1):\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_len = output_len\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        \n",
    "        self.model = Causal_1dConv(in_channels = in_channels,\n",
    "                                   out_channels = out_channels * output_len,\n",
    "                                   kernel_size = kernel_size, \n",
    "                                   bias = False, stride = stride,\n",
    "                                   dilation = dilation)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        assert x.shape[1] == self.in_channels, 'not proper number of channels'\n",
    "        seq_len = x.shape[2]\n",
    "        \n",
    "        # should be 3 dim\n",
    "        preds = self.model(x)\n",
    "        print(preds.shape)\n",
    "        # needs to be expanded to 4 in order to overlay on top of y_true\n",
    "        # batch_size, channels, seq_len, output_len\n",
    "# uncomment this\n",
    "        #         preds = preds.view(batch_size,self.out_channels,-1,self.output_len) \n",
    "# UNCOMMENT THIS\n",
    "        \n",
    "        # padding is independent of dilation \n",
    "        padding = (int(self.kernel_size - 1 ), 0)\n",
    "        x = torch.nn.functional.pad(x, padding)\n",
    "        y_true = torch_running_view(x,self.output_len)\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150., 160., 170.],\n",
      "         [105., 115., 125., 135., 145., 155., 165., 175.]]])\n",
      "shape:    torch.Size([2, 2, 8])\n",
      "torch.Size([1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 8]), torch.Size([1, 2, 6, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "print(data)\n",
    "print('shape:   ',data.shape)\n",
    "\n",
    "\n",
    "chans =  data.shape[1]\n",
    "kernel_size = 2\n",
    "output_len = 4\n",
    "dilation = 1\n",
    "\n",
    "autoencoder = Autoencoder(in_channels = chans, stride=1,\n",
    "                          out_channels = chans,\n",
    "                          output_len = output_len,\n",
    "                          kernel_size = kernel_size,\n",
    "                         dilation=dilation)\n",
    "\n",
    "\n",
    "autoencoder.model.conv.weight.data = torch.ones(chans*output_len,chans,kernel_size)\n",
    "# autoencoder.model.conv.weight.data[1,:,:] = 2\n",
    "preds, y_true = autoencoder.predict(data[0].view(1,2,-1))\n",
    "\n",
    "preds.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.model.conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.model.conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 0.,  5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = data[0]\n",
    "padding = (int((kernel_size-1)*dilation),0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f25951b4356d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# batch, chans, seqlen, output_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "# batch, chans, seqlen, output_len\n",
    "preds[0,0,:,:]#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# its repeating information because everything is a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "         [  5.,  30.,  70., 110., 150., 190., 230., 270.]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.]],\n",
       "\n",
       "         [[  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.],\n",
       "          [  5.,  30.,  70., 110., 150., 190., 230., 270.]]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an extra dim to split the channels from output dim (remember we're having the conv layer give us num_chan * output_dim)\n",
    "test = preds.view(1,chans,output_len,data.shape[-1])#.permute((0,1,3,2))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 5., 5., 5.], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch, chan, output_index, current time\n",
    "test[0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.,   5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.,  30.],\n",
       "          [ 70.,  70.,  70.,  70.],\n",
       "          [110., 110., 110., 110.],\n",
       "          [150., 150., 150., 150.],\n",
       "          [190., 190., 190., 190.],\n",
       "          [230., 230., 230., 230.],\n",
       "          [270., 270., 270., 270.]],\n",
       "\n",
       "         [[  5.,   5.,   5.,   5.],\n",
       "          [ 30.,  30.,  30.,  30.],\n",
       "          [ 70.,  70.,  70.,  70.],\n",
       "          [110., 110., 110., 110.],\n",
       "          [150., 150., 150., 150.],\n",
       "          [190., 190., 190., 190.],\n",
       "          [230., 230., 230., 230.],\n",
       "          [270., 270., 270., 270.]]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to match the shape of y_true we need to swap the axis (using permute) \n",
    "# each row is all the same because all four of the kernels (or 8 if you consider both channels) are seeing\n",
    "# the same input at that time step and all the kernels are also identical\n",
    "# ideally the output kernels would learn to be different based on what the input is (which right now is not only 2 time steps of two channels\n",
    "# so not a whole lot of information for it to expand to 8 values)\n",
    "\n",
    "# I guess if we had multiple layers, only the output would need to receive this reshaping treatment\n",
    "test2 = test.permute((0,1,3,2))\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 0.,  5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = data[0]\n",
    "padding = (int((kernel_size-1)*dilation),0)\n",
    "signal = torch.nn.functional.pad(signal, padding)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0., 10., 20.],\n",
       "          [ 0., 10., 20., 30.],\n",
       "          [10., 20., 30., 40.],\n",
       "          [20., 30., 40., 50.],\n",
       "          [30., 40., 50., 60.],\n",
       "          [40., 50., 60., 70.]],\n",
       "\n",
       "         [[ 0.,  5., 15., 25.],\n",
       "          [ 5., 15., 25., 35.],\n",
       "          [15., 25., 35., 45.],\n",
       "          [25., 35., 45., 55.],\n",
       "          [35., 45., 55., 65.],\n",
       "          [45., 55., 65., 75.]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d758a95b0509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "(batch_size,self.out_channels,-1,self.output_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-4, 3], but got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-22c698e32fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-4, 3], but got 4)"
     ]
    }
   ],
   "source": [
    "preds.view(1,2,data.shape[-1],-1).transpose(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]],\n",
       "\n",
       "       [[3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        ...,\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.],\n",
       "        [3., 2., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat((torch.ones(1, 24, 24),\n",
    "               torch.ones(1, 24, 24)*2,\n",
    "               torch.ones(1, 24, 24)*3), 0)\n",
    "\n",
    "permute = [2, 1, 0]\n",
    "x = x[permute, :, :]\n",
    "y = x.numpy()\n",
    "c, h, w = y.shape\n",
    "y = y.transpose(1, 2, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         ...,\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.]],\n",
       "\n",
       "        [[2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         ...,\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0., 10.],\n",
       "          [10., 20.],\n",
       "          [20., 30.],\n",
       "          [30., 40.],\n",
       "          [40., 50.],\n",
       "          [50., 60.],\n",
       "          [60., 70.]],\n",
       "\n",
       "         [[ 0.,  5.],\n",
       "          [ 5., 15.],\n",
       "          [15., 25.],\n",
       "          [25., 35.],\n",
       "          [35., 45.],\n",
       "          [45., 55.],\n",
       "          [55., 65.],\n",
       "          [65., 75.]]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.],\n",
      "         [  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.]],\n",
      "\n",
      "        [[100., 110., 120., 130., 140., 150., 160., 170.],\n",
      "         [105., 115., 125., 135., 145., 155., 165., 175.]]])\n",
      "shape:    torch.Size([2, 2, 8])\n",
      "reshaped: torch.Size([2, 2, 6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10., 20.],\n",
       "        [10., 20., 30.],\n",
       "        [20., 30., 40.],\n",
       "        [30., 40., 50.],\n",
       "        [40., 50., 60.],\n",
       "        [50., 60., 70.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def torch_running_view(tensor,window,axis=-1):\n",
    "    shape = list(tensor.shape)\n",
    "    shape[axis] -= (window -1)\n",
    "    assert(shape[axis]>0)\n",
    "    return torch.as_strided(tensor, shape + [window], \n",
    "                            tensor.stride() + (tensor.stride()[axis],))\n",
    "\n",
    "# data = torch.arange(10).view(2,1,-1)\n",
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "print(data)\n",
    "print('shape:   ',data.shape)\n",
    "\n",
    "reshaped_data = torch_running_view(data,3)\n",
    "print(\"reshaped:\",reshaped_data.shape)\n",
    "reshaped_data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([ 0.,  1.,  3.,  6., 10., 15., 20., 25., 30., 35., 40.],\n",
      "       grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(20).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.,  15.,  25.,  35.,  45.,  55.,  65.,  75.,  85.,  95., 105.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(20).float()\n",
    "\n",
    "data *10 +5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10., 20., 30., 40., 50., 60., 70.],\n",
       "        [ 5., 15., 25., 35., 45., 55., 65., 75.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros((2,2,8)) + torch.arange(8).float() *10\n",
    "data[:,1] = data[:,1] + 5\n",
    "data[1] = data[1] + 100\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100., 110., 120., 130., 140., 150., 160., 170.],\n",
       "        [105., 115., 125., 135., 145., 155., 165., 175.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "as_strided(): argument 'stride' (position 3) must be tuple of ints, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2385af3d5c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_strided\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: as_strided(): argument 'stride' (position 3) must be tuple of ints, not int"
     ]
    }
   ],
   "source": [
    "torch.as_strided(data[0,0],(2,-1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_view(arr, window, axis=-1):\n",
    "    \"\"\"\n",
    "    return a running view of length 'window' over 'axis'\n",
    "    the returned array has an extra last dimension, which spans the window\n",
    "    \"\"\"\n",
    "    shape = list(arr.shape)\n",
    "    shape[axis] -= (window-1)\n",
    "    assert(shape[axis]>0)\n",
    "    return np.lib.index_tricks.as_strided(\n",
    "        arr,\n",
    "        shape + [window],\n",
    "        arr.strides + (arr.strides[axis],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_rolling_window(x, window_size, step_size=1):\n",
    "    # unfold dimension to make our rolling window\n",
    "    return x.unfold(0,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 5],\n",
       "         [1, 6],\n",
       "         [2, 7],\n",
       "         [3, 8],\n",
       "         [4, 9]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(10).view(2,-1)\n",
    "\n",
    "pytorch_rolling_window(data,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.arange(10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4],\n",
       "       [4, 5],\n",
       "       [5, 6],\n",
       "       [6, 7],\n",
       "       [7, 8],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_view(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to CNN (must read if need clarification on some of the terminology) https://arxiv.org/pdf/1603.07285.pdf\n",
    "\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last decade, we witnessed great strides in the field of computer vision through the utilization and development of deep convolutional neural network architectures. Thus, much attention has been given to the development and manipulation of these convolutional models both in terms of theory and ease of implementation/prototyping (think PyTorch, TensorFlow, etc).\n",
    "\n",
    "However, time series modeling can also use this and is really can just be thought of as a simpler 1D version of the 2D.  We would like to  \n",
    "\n",
    "This tutorial will focus on providing a top down approach to building a 1D convolutional model in PyTorch.  We will walk through the arithmetic of the different hyperparameters of such models in order to provide a solid foundation for more sophisticated hypertuning of these parameters to build highly complex and powerful neural network architectures for time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's `nn.Conv1d` Class\n",
    "\n",
    "We will focus on a code-first, top-down approach and fill in theoretical details as needed. The hope is that by the end of this you will feel comfortable working with all the levers made available by a modern, open-source, machine learning libraries. Thus, we begin with noting that PyTorch provides a native class for a 1D convolutional layer:\n",
    "\n",
    "`torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "We begin with the required arguments (`in_channels`, `out_channels`, `kernel_size`) of the `torch.nn.Conv1d` class before moving on to the keyword arguments.  The documentation defines them in the following way:\n",
    "```\n",
    "in_channels (python:int) – Number of channels in the input image\n",
    "out_channels (python:int) – Number of channels produced by the convolution\n",
    "kernel_size (python:int or tuple) – Size of the convolving kernel\n",
    "```\n",
    "Notice the slight Freudian slip of the architects assuming that the input would be an \"image.\"  Since we're undyingly grateful for the work they've done, we'll let this slide and instead focus on discussing the implication of being able to specify the number of channels in a \"so-called\" 1D model. \n",
    "\n",
    "## The Univariate Problem: 1 channel in, 1 channel out\n",
    "\n",
    "Again, we're taking a build-first approach so let's just start with a simple example: 1 channel in, 1 channel out, and a kernel_size of 5.  This is a realistic starting point since, oftentimes, a time series problem is framed such that a univariate (single channel input) dataset is used to predict the future value of itself (single channel output).  (Aside: we will also remove the bias term from the model for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 1, kernel_size=(5,), stride=(1,), bias=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = torch.nn.Conv1d(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False)\n",
    "conv1d_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we've just created our first 1D convolutional layer that inherently leverages all the power of PyTorch's autograd functionality, GPU utilization, etc.  But... what was actually created? Well let's take a look at the kernel (or weight matrix if you prefer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.0071,  0.1819,  0.4029, -0.4130,  0.0206]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, PyTorch has randomly generated 5 numbers to act as our initial weight tensor.  However, if you'd like to use your own weight initialization logic you can overlay this by accessing the values of the tensor using the `data` attribute.  Let's say we'd previously optimized this model and knew that it should simply take an unweighted moving average of the last 5 data points. We would modify all the weights to be 1/5 like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use similar logic to overlay any number of weight initialization protocols.\n",
    "\n",
    "OK, so we have a model (which takes an unweighted average over the 5 most recent data points), let's create a fake stream of time series data and pass it in to see what happens (of course, reshaping to the expected dimensions of `[batch_size, channels_in, seq_length]` and make sure that the type is `float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 14])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[5.2000, 5.8000, 5.8000, 5.6000, 5.6000, 5.8000, 5.6000, 5.4000,\n",
       "          6.8000, 5.8000]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([4, 9, 3, 6, 4, 7, 9, 2, 6, 5, 6, 8, 9, 1, 4]).float()\n",
    "# reformat data [batch, channels, seq_len]\n",
    "X = data.view(1,1,-1)[:,:,:-1]\n",
    "print(X.shape)\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "print(output.squeeze().shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY ISN'T IT CAUSAL? Seems like it is if set up like this\n",
    "\n",
    "the average of [0., 1., 2., 3., 4] is 2 and likewise is 3 of [1., 2., 3., 4, 5].  So clearly its taking the first 5 then shifting 1 to the right...etc.  I can set y_true to be 5 and 6 and let it modify it's weights but to me this proves that it's doing what we hope it would unless I'm missing something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([0., 1., 2., 3., 4., 5.])\n",
      "y_true: tensor([5., 6.])\n",
      "y_pred: tensor([2., 3.], grad_fn=<RoundBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(7).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)[:,:,:-1]\n",
    "y_true = data.squeeze()[kernel_size:]\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {torch.round(output.view(-1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = torch.nn.Conv1d(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False,dilation=2)\n",
    "conv1d_layer.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([0.0000, 0.2000, 0.6000, 1.2000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000,\n",
      "        7.0000, 8.0000], grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(11).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wavenets implementation is easier, maybe don't introduce it until later\n",
    "\n",
    "https://github.com/NVIDIA/nv-wavenet/blob/master/pytorch/wavenet.py#L23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolution with the option to be causal and use xavier initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 dilation=1, bias=True, w_init_gain='linear', is_causal=False):\n",
    "        super(Conv, self).__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    dilation=dilation, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        if self.is_causal:\n",
    "                padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
    "                signal = torch.nn.functional.pad(signal, padding) \n",
    "        return self.conv(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfronczak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size =5\n",
    "conv1d_layer = Conv(in_channels = 1, out_channels = 1, \n",
    "                               kernel_size =kernel_size, bias = False,dilation=1,is_causal=True)\n",
    "conv1d_layer.conv.weight.data = torch.ones(1,1,5) / 5\n",
    "conv1d_layer.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_true: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "y_pred: tensor([0.0000, 0.2000, 0.6000, 1.2000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000,\n",
      "        7.0000, 8.0000], grad_fn=<ViewBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 11]), torch.Size([1, 1, 11]), torch.Size([11]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(11).float()\n",
    "# reformat data\n",
    "X = data.view(1,1,-1)\n",
    "y_true = data.squeeze()\n",
    "\n",
    "# pass into our convolutional layer\n",
    "output = conv1d_layer(X)\n",
    "\n",
    "print(f'     X: {X.squeeze()}\\ny_true: {y_true}\\ny_pred: {(output.view(-1))}')\n",
    "print('\\n')\n",
    "output.shape, X.shape, y_true.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this guy goes into how to restructuredata if you have multivariate, stacked output https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0,2,4,6,8])+1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh the simplicity of PyTorch.  All we needed to do was pass our input data in as an argument to the model we created and it generates the output series based on the weights stored (or optimized as that may be the case).  But you may notice that there are only 11 values in the output series whereas our original dataset had 15.  Recall that we are taking a moving average of 5 data points which means that, in order to make an inference at a particular point in time, we need to know the previous 4 values.  Thus, our first output can only come once we have \"seen\" 5 data points. The figure provides a visual of how a 1d convolutional kernel slides along the time axis of our input series to provide a single output value at every timestep after skipping the first 4.  Again, PyTorch handles all of this logic internally for us but understanding the expected dimensional output is still very important for model scoring, prototyping, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Picture of sliding conv on time axis\n",
    "\n",
    "11 small pictures with shading to show where kernel is focusing, and then one of them will be a zoomed in to show the math\n",
    "\n",
    "input\n",
    "|\n",
    "kernel\n",
    "|\n",
    "output\n",
    "\n",
    "note the directionality of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multivariate Problem: `n` input channels, `m` output channels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For example, let's say that we were interest in predicting the future value of a particular stock and only wanted to use past-values of that stock to do so.  That would mean that both `in_channels` and `out_channels` would be set to 1.  However, if we wanted to use the 5 most similar company's stock prices to predict the value of a single stock then `out_channels` would stay \n",
    "\n",
    "Ok enough talk, let's start building...\n",
    "\n",
    "the input of a 1D model would ever have more  why a time series would ever be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The bread and butter of neural networks is affine transformations: a vector\n",
    "is received as input and is multiplied with a matrix to produce an output (to\n",
    "which a bias vector is usually added before passing the result through a nonlinearity).\" https://arxiv.org/pdf/1603.07285.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic 1d Conv\n",
    "\n",
    "Look at what the basic output looks like\n",
    "\n",
    "1 channel in and 1 channel out, and a kernel length of 5,\n",
    "\n",
    "no extra striding (stride =1), no extra padding (paadding = 0), no dilation (dilation =1)\n",
    "\n",
    "note that we are only required to give `in_channels, out_channels, kernel_size`. left the others there with their defaults (except bias, we're turning that off for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2755,  0.1295, -0.0642,  0.2511,  0.3457]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =5,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some simple data, 2 channels with sequence length: 15 (obviously not very interesting data here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones((1,15))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets grab the first channel and add 2 dummy dimensions to it since that's what's expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = data[0].view(1,1,-1)\n",
    "# inp[0,0,-1] = 4\n",
    "# inp[0,0,4] = 4\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it into the model and get 11 numbers out which makes sense because the first four inputs didn't have enough past history to make a prediction (kernel size = 5) so it's only at the 5th element that we get our first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376, 0.9376,\n",
       "          0.9376, 0.9376, 0.9376]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the value, it's not surprisingly constant but its also just the sum of all the elements in the kernel since our data is simply ones ($\\sum_{i=0}^k w_i$ where $k$ is kernal length and $w_i$ refers to each weight-element in the kernel). \n",
    "\n",
    "One can access the kernel weights with the `weight` attribute for the layer in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6312, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### kernel is not a multiple of sequence length\n",
    "\n",
    "What happens if we change the kernal length to be a non-multiple of the sequence length (k = 4 and seq_len = 15)\n",
    "\n",
    "Not a problem since our stride is 1 and we don't have any padding, it just follows the same pattern above where the first k-1 points don't recieve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.4973, -0.2145,  0.1587, -0.0901]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =4,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again since the kernel is overlaid on top of the sequence, the first 3 values in the sequence don't have corresponding outputs from the model (since it is a requirement to have 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6433, -0.6433, -0.6433, -0.6433, -0.6433, -0.6433, -0.6433,\n",
       "          -0.6433, -0.6433, -0.6433, -0.6433, -0.6433]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inputs\n",
    "\n",
    "So what happens when we just change the number of input channels but still expect 1 channel in the output. We see that the weight tensor now has 2 kernels of length 5 (so the shape is (1,2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones((2,15))\n",
    "data[1] += 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 5.5815e-02,  6.2070e-02,  1.1655e-01, -1.6826e-01, -2.7581e-01],\n",
      "         [ 2.0035e-01, -1.8993e-04,  1.6400e-01,  4.2376e-02,  1.4429e-01]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 2, out_channels = 1, kernel_size =5,\n",
    "                        stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 2 dummy dimension to the tensor since that's what's expected by torch (shape = (1,2,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = data.view(1,2,15)\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920, 0.8920,\n",
       "          0.8920, 0.8920, 0.8920]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are constant again (as to be expected) and simply the sum of kernel1 convolving the first series (of ones) and kernel2 convolving the second series (of 2s).  Again, the first convolution is just the sum of the weight elements in the kernel ($\\sum_{i=0}^k w_i$) whereas the the second\n",
    "\n",
    "Note that it's just $\\sum_{i=0}^k 1*w_i$ where $k$ is kernal length and $w_i$ refers to each weight. It's mult by 1 right now since our series is just ones [1,1,1....,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8920, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel1 = model.weight[0,0]\n",
    "kernel2 = model.weight[0,1]\n",
    "\n",
    "kernel1.sum() * 1 + kernel2.sum() * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
       "          2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(1,1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride is how many steps to take before making a new calculation (if you say 0 it will just stay in one spot and obviously kill the kernel you're working on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.4267,  0.4077, -0.1108,  0.2961,  0.1737]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8097, 0.8097, 0.8097, 0.8097, 0.8097, 0.6989, 0.6989, 0.6989,\n",
       "          0.6989, 0.6989]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size =5,\n",
    "                stride=1, padding=0, dilation=2, groups=1, \n",
    "                bias=False, padding_mode='zeros')\n",
    "\n",
    "print(model.weight)\n",
    "model(a.view(1,1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4398, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight[0,0,1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0668, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight[].sum()*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3823)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0.2745, -0.1809,  0.0806, -0.3044, -0.2521]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2890)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0.2901,  0.1385, -0.2273, -0.1210,  0.2087]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19569999999999999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.3823 + 0.2890*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9473)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0807,  0.1279, -0.1774, -0.0982, -0.0345]).sum() + 2 * torch.tensor([-0.0052,  0.0016,  0.1569,  0.2386,  0.1325]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
